---
title: 'Towards Automated Circuit Discovery - visual breakdown'
author: Aleks Hiidenhovi
published: 2024-09-10
---

import CodeEditor from '@/components/CodeEditor.tsx'
import PhotoGallery from '@/components/PhotoGallery.tsx'

## Introduction

(*Course project for the AI Safety Fundamentals course, summer 2024 edition*)

The black-box nature of deep learning models has been a long-standing issue in the field of AI. While these models have shown impressive performance in a wide range of tasks, the research community still lacks a comprehensive understanding of their inner workings. As frontier models are automizing more and more of our daily lives and as society gives more control to powerful AI systems, we need ways to guarantee that these systems are safe and reliable. 

Mechanistic interpretability is a small subfield of AI safety research that aims to address these concerns by reverse engineering neural networks from their learned weights to human-understandable algorithms. One of the key concepts in mechanistic interpretability is the idea of *circuits*, which are subgraphs of a neural network that are responsible for performing a particular task. In this blog post, I will present the paper [*Towards Automated Circuit Discovery for Mechanistic Interpretability*](https://arxiv.org/pdf/2304.14997) which studies automated ways of uncovering circuits in small transformer models. The core motivation for this paper is to take the first steps in automating circuit discovery which has previously involved extensive manual handwork and iteration.

But first, let's take a step back and take a closer look on the fundamentals of circuits.

## What are circuits?
To understand the role of circuits in neural networks, we first need to understand the concept of a neural network *feature*. [Olah et al.](https://distill.pub/2020/circuits/zoom-in) define a feature to be a low-level fundamental unit of a neural network. As a concrete example, a convolutional neural network trained to classify images might learn to detect edges, lines and textures.

<PhotoGallery client:load images={[
  {src: "/images/circuits-curve-1.png", alt: "First curve feature"},
  {src: "/images/circuits-curve-2.png", alt: "Second curve feature"},
  {src: "/images/circuits-curve-3.png", alt: "Third curve feature"},
]} caption='Curve detector features: Olah, et al., "Zoom In: An Introduction to Circuits"' />

According to [Olah et al.](https://distill.pub/2020/circuits/zoom-in), a *circuit* is a subgraph of a neural network. It consists of a set of fundamentals units called *features* that are connected with *weighted edges*.

In transformer models specifically, a feature could be as small as a single neuron, but more often we are interested in larger sub-units of the network, such as attention heads or MLP layers. The invidual units, and moreover the whole networks, are connected together by the [*residual stream*](https://transformer-circuits.pub/2021/framework/index.html#residual-comms).


## Manual circuit discovery process
Before this paper was announced, the general methodology for identifying circuits involved the following 3 manual steps:
1. **Task definition:** Recognizing a behavior or task that the model displayes, creating a dataset that reproduces that isolates and reproduces that behavior, and choosing a metric to measure how well the model performs at the task.
2. **Interpretation scoping:** Deciding the level of granularity (e.g. attention heads/MLPs, individual neurons, etc...) at which the network should be interpreted in.
3. **Manual discovery:** Performing extensive patching experiments in order to remove as many unnecessary sub-units and edges as possible.

The paper introducess an algorithm called Automatic Circuit DisCovery (ACDC), the goal of which is to fully automate step 3 of the process.

## The ACDC algorithm
On the high-level, ACDC finds a circuit by iteratively pruning unimportant nodes from the network. More formally, the algorithm executes the following steps:
1. **Network sorting:** The network is topologically sorted in reverse, i.e. from output to input nodes.
2. **Iterative pruning:** The algorithm iterates every edge in the graph. It prunes a node if it is considered "unimportant" according to the threshold function (more on that next)
3. **Tree-shaking:** Some nodes in the network might end up in a dead end if all its parent edges are cut off. Tree-shaking removes these "dead nodes".

The final result is a subgraph of the network that only has nodes with a direct connection from the input, all the way to the output node.

![How ACDC works](https://drive.google.com/uc?id=1_UQWmoVuE61pC-PodD3SihNU7CnqzR3L)

More detailed information about the algorithm can be found Section 3 from the [paper](https://arxiv.org/pdf/2304.14997), and in the "Experiment section" of this post.

## Experiment

### Task definition: Greater Than
The paper presents 6 different circuit discovery tasks that they attempted to automate with ACDC. In this experiment, we focus on the "Greater-Than" -task, first introduced by Hanna et al. in the paper [*How does GPT-2 compute greater-than?*](https://arxiv.org/pdf/2305.00586).

The Greater-Than -task involves implicitly prompting a model to output greater number than the reference number, based on its context. More concretely, consider the following sentence and think about the possible outputs:

> The war lasted from 1720 to the year 17

From the context, we do NOT know which exact war is being referred here. But in terms of language restrictions, numbers between 1-20 should not be possible answers, while anything greater or equal to 21 would be a plausible answer.

### Models
We use HuggingFace's GPT-2 model with language modeling head (LMHead). The LM Head means that the model has a linear layer at the end, the weights of which are tried to the embedding layer. This layer outputs the final logits.

<CodeEditor client:only lang={"shell"} code={`%pip install transformer_lens
%pip install circuitsvis
%pip install networkx -q`} />

## Conclusion

### Limitations

### Upcoming frontiers from automated interpretability

## References