---
title: 'Towards Automated Circuit Discovery - visual breakdown'
author: Aleks Hiidenhovi
published: 2024-09-10
---

import CodeEditor from '@/components/CodeEditor.tsx'
import PhotoGallery from '@/components/PhotoGallery.tsx'

## Introduction

(*Course project for the AI Safety Fundamentals course, summer 2024 edition*)

The black-box nature of deep learning models has been a long-standing issue in the field of AI. While these models have shown impressive performance in a wide range of tasks, the research community still lacks a comprehensive understanding of their inner workings. As frontier models increasingly automate various aspects of our daily lives and as society gives more control to powerful AI systems, we need ways to guarantee that these systems are safe and reliable. 

Mechanistic interpretability is a subfield of AI safety research that aims to address these concerns by reverse engineering neural networks from their learned weights to human-understandable algorithms. One of the key concepts in mechanistic interpretability is the idea of *circuits*, which are subgraphs of a neural network that are responsible for performing a particular task. In this blog post, I will present the paper [*Towards Automated Circuit Discovery for Mechanistic Interpretability*](https://arxiv.org/pdf/2304.14997) which studies automated ways of uncovering circuits in small transformer models. The core motivation for this paper is to take the first steps in automating circuit discovery which has previously involved extensive manual handwork and iteration.

But first, let's take a step back and take a closer look on the fundamentals of circuits.



## What are circuits?
To grasp the role of circuits in neural networks, we must first explore the concept of a neural network *feature*. I find that visual examples from computer vision tasks convey the concept of features and circuits in a neural network in an intuitive way, compared to abstract language tasks. We move onto analyzing language models after this short tangent.

### Circuits in vision models
[Olah et al.](https://distill.pub/2020/circuits/zoom-in) define a feature to be a low-level fundamental unit of a neural network. As a concrete example, a convolutional neural network trained to classify images might learn to detect edges, lines and textures.

<PhotoGallery client:load images={[
  {src: "/images/circuits-curve-1.png", alt: "First curve feature"},
  {src: "/images/circuits-curve-2.png", alt: "Second curve feature"},
  {src: "/images/circuits-curve-3.png", alt: "Third curve feature"},
]} caption='Curve detector features: Olah, et al., "Zoom In: An Introduction to Circuits"' />

When these low-level features interact with each other, they form a *circuit*. More formally:

> A circuit is neural network subgraph, consisting of *features* that are connected with *weighted edges*.

A high-level circuit for detecting curves could consist of detectors for simpler curves and lines, which would be combined to detect more complex curves, as shown in the picture below.

<PhotoGallery client:load images={[
  {src: "/images/curve-circuit.png", alt: "High-level curve detector circuit"},
]} caption='High-level curve detector circuit: Olah, et al., "Zoom In: An Introduction to Circuits"' />

### Circuits in language models
Features and circuits in language models tend to be a bit more abstract than in computer vision models but the concept is the same: lower-level features interact to form higher-level features which together form a circuit. For example, low-level language model features could be for example "subject", "verb", or "object", and a circuit could then combine these features to detect a sentence structure.

The paper presents 6 different tasks, for which the authors attempted to find the corresponding circuits. We focus here on two of the simpler tasks: the "Greater-Than" -task and the "Indirect Object Identification (IOI)" -task.

#### Greater-Than
"Greater-Than" -task was first introduced by Hanna et al. in the paper [*How does GPT-2 compute greater-than?*](https://arxiv.org/pdf/2305.00586). It involves prompting the model to output a greater number than the reference number, based on its context. More concretely, consider the following sentence and think about what the possible outputs could be:

> The war lasted from 1720 to the year 17

From the context itself, we do NOT know which exact war is being referred here. But in terms of context of the sentence, the war should have ended in years 1721 and onwards, not in the years prior to that. Thus, the model is expected to output a number greater than 20 to succeed in the task.

#### Indirect Object Identification
Indirect Object Identification (IOI) -circuit was found and presented by Wang et al. in the paper [*Interpretability in the Wild*](https://arxiv.org/pdf/2211.00593). The task involves the target of an action in the sentence. The authors analyzed the following sentence:

> When Mary and John went to the store, John gave a drink to

The model should infer from the context that the drink was given to Mary, not to John. Smaller language models can sometimes struggle with this task, as the target of the action is not explicitly mentioned in the sentence and the model may predict the wrong target.

Now that we have a basic understanding of what circuits are and some example behaviors that they attempt to capture, we can start to think about how we can discover them in neural networks.



## Manual circuit discovery process
Coming up with a circuit in your head is one thing, but how do we actually find them in a neural network? Before this paper was published, the general methodology for identifying circuits involved the following 3 manual steps:
1. **Task definition:** Recognizing a behavior or task that the model displays, creating a dataset that isolates and reproduces that behavior, and choosing a metric to measure how well the model performs at the task.
2. **Interpretation scoping:** Deciding the level of granularity (e.g. attention heads/MLPs, individual neurons, etc...) at which the network should be interpreted in.
3. **Manual discovery:** Performing extensive patching experiments in order to remove as many unnecessary sub-units and edges as possible.

<PhotoGallery client:load images={[
  {src: "/images/manual-circuit-discovery-process.png", alt: "Manual circuit discovery process -illustrated"},
]} caption='Manual circuit discovery process -illustrated' />

Manually completing this process takes a lot of time and effort, and success is not guaranteed. A more automated approach would increase the iteration speed and enable researchers to discover circuits more efficiently.

The paper in question attempts to respond to this challenge by introducing a new algorithm called Automatic Circuit DisCovery (ACDC), the goal of which is to fully automate step 3 of the process.



## Towards automation: The ACDC algorithm
At a high-level, ACDC finds a circuit by iteratively pruning unimportant nodes from the network. More formally, the algorithm executes the following steps:

1. **Network sorting:** The computational graph of the network is created, sorted topologically, and then reversed (from output nodes to input nodes).
2. **Iterative pruning:** The algorithm begins the pruning step, starting from the output node. It prunes an edge if it is considered "unimportant" according to the chosen threshold metric.
3. **Tree-shaking:** Some nodes in the network might end up in a dead end if all its parent edges are cut off. Tree-shaking removes these "dead nodes".

<PhotoGallery client:load images={[
  {src: "/images/acdc-full-pruning-process.png", alt: "ACDC end-to-end pruning process"},
]} caption='ACDC end-to-end pruning process, extension of Figure 2 in the paper' />

There is a lot of jargon and small hidden details within these steps, so let's break them down a bit more.

### Network sorting
The first step involves sorting the network topologically in reverse order, i.e. according to the computation order of the nodes but starting from the output. This is done to ensure that the algorithm prunes the network in the correct order, starting from the output nodes and moving towards the input nodes.

<PhotoGallery client:load images={[
  {src: "/images/acdc-topological-sort.png", alt: "ACDC subgraph, topologically sorted"},
]} caption='ACDC subgraph, topologically sorted' />

### Iterative pruning
Now that the network is sorted, the algorithm can start the pruning process. The algorithm iteratively prunes edges from the network one node at a time, starting from the output node. The algorithm decides to prune an edge using the following formula:

$$
\begin{equation}
H_{new} = H \setminus \{ w \rightarrow v \}
\end{equation}
$$
$$
\begin{equation}
D_{KL}(G \, \Vert \, H_{new}) - D_{KL}(G \, \Vert \, H) < \tau
\end{equation}
$$

This formula may look a bit daunting if you're not familiar with the notation, but the concepts behind it are quite simple. In essence, the formula calculates the difference between two different result distribution $G$ and another result distribution $H$ and then checks whether the difference is less than a threshold $\tau$. 
* $H_{new}$ is the network with the edge $w \rightarrow v$ pruned.
* [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) $D_{KL}$ measures how much one probability distribution differs from another.
* $\tau$ is user-defined hyperparamater that controls the aggressiveness of the pruning. If the difference is less than $\tau$, the edge is pruned. This means that removing the edge does not significantly change the output of the network.

<PhotoGallery client:load images={[
  {src: "/images/acdc-pruning.png", alt: "ACDC pruning step"},
]} caption='ACDC pruning step' />

After pruning an edge, the algorithm proceeds to the next node in the topologically sorted network and repeats the process. This iterative pruning continues until the algorithm reaches the input node.

### Tree-shaking
The circuit needs to have a directed connection from the input to the output node. This is not necessarily true for all nodes after the pruning step, as some nodes might have lost all their parent connections during the pruning process. Tree-shaking removes these "dead nodes" from the network.

<PhotoGallery client:load images={[
  {src: "/images/acdc-tree-shaking.png", alt: "ACDC tree-shaking step"},
]} caption='ACDC tree-shaking step' />

The final result is a subgraph of the network that only has nodes with a direct connection from the input, all the way to the output node. And more importantly, this subgraph is the circuit that the algorithm found to be responsible for the task at hand!


## Conclusion
ACDC is an exciting first step towards automating the circuit discovery process for neural networks. Developing methods to automate this process is crucial for scaling mechanistic interpretability work to larger models and more complex tasks. 

### Limitations
Although this method represents the first step towards automatic circuit discovery process, it is not without its limitations. 

First of all, the algorithm is not guaranteed to find the optimal circuit. This is due to the fact that the pruning process is a so called [*greedy algorithm*](https://en.wikipedia.org/wiki/Greedy_algorithm), where it makes the best decision at each step without considering the global optimal solution. 

The algorithm is also computationally expensive to run, since it requires multiple forward passes through the network for each edge pruning step. This makes it difficult to apply to larger networks than the ones used in the paper.

### Upcoming frontiers for automated interpretability
I believe that there is a lot that can be done to extend the work presented in this paper. For example, different pruning strategies could be explored that would converge closer to the global optimal circuit. Additionally, this version of the algorithm is quite resource-intensive, so finding ways to make it more efficient would make it much more applicable to larger networks, both in terms of costs and time.

This truly seems like a great time for anyone interested in the field to jump in start contributing!